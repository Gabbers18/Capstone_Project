# -*- coding: utf-8 -*-
"""Topic_Modeling_Part1Week9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SM2noEUrSsZa1D30-4oj-Zh6u5ULdJD6

**Part 1 Week 9 Topic Modeling**
"""

!pip install newsapi-python
!pip install git+https://github.com/rwalk/gsdmm.git

import re
import pandas as pd
import gensim
import numpy as np
from gsdmm import MovieGroupProcess
from textblob import TextBlob
from gensim.parsing.preprocessing import remove_stopwords
from gensim.parsing.preprocessing import STOPWORDS
from gensim.corpora.textcorpus import lower_to_unicode
from gensim import corpora, models
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk import word_tokenize, sent_tokenize
ps = nltk.porter.PorterStemmer()

from nltk.stem import WordNetLemmatizer

nltk.download('omw-1.4')
nltk.download('averaged_perceptron_tagger')
from collections import Counter
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

import unicodedata

from newsapi import NewsApiClient

# Init
# newsapi = NewsApiClient(api_key='insert your api key here')

ng = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')

ng.head()

"""1. Randomly select 40% of the posts in the newsgroup dataset."""

# Referring to the documentation, I set the "seed" to 123 to make this sample reproducable.
ng_sample = ng.sample(frac = .40, random_state = 123)

ng_sample.head()

"""2. Create descriptive statistics."""

# I found the number of total articles.
total_articles = ng_sample.shape[0]
total_articles

# I counted the number of articles within each group (target names).
# I referred to a past assignment to find how to do this.
articles_per_group = ng_sample['target_names'].value_counts()
articles_per_group

"""3. Prepare the text for topic modeling."""

articles = []
for article in ng_sample['content'].tolist():
  if article:
    articles.append(article)

articles

def top_words(cluster_word_distribution, top_cluster, values):
    for cluster in top_cluster:
        sort_dicts = sorted(cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]

        print("\nCluster %s , %s"%(cluster, sort_dicts))

# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python
contractions = {
"ain't": "am not",
"aren't": "are not",
"can't": "cannot",
"can't've": "cannot have",
"'cause": "because",
"could've": "could have",
"couldn't": "could not",
"couldn't've": "could not have",
"didn't": "did not",
"doesn't": "does not",
"don't": "do not",
"hadn't": "had not",
"hadn't've": "had not have",
"hasn't": "has not",
"haven't": "have not",
"he'd": "he would",
"he'd've": "he would have",
"he'll": "he will",
"he's": "he is",
"how'd": "how did",
"how'll": "how will",
"how's": "how is",
"i'd": "i would",
"i'll": "i will",
"i'm": "i am",
"i've": "i have",
"isn't": "is not",
"it'd": "it would",
"it'll": "it will",
"it's": "it is",
"let's": "let us",
"ma'am": "madam",
"mayn't": "may not",
"might've": "might have",
"mightn't": "might not",
"must've": "must have",
"mustn't": "must not",
"needn't": "need not",
"oughtn't": "ought not",
"shan't": "shall not",
"sha'n't": "shall not",
"she'd": "she would",
"she'll": "she will",
"she's": "she is",
"should've": "should have",
"shouldn't": "should not",
"that'd": "that would",
"that's": "that is",
"there'd": "there had",
"there's": "there is",
"they'd": "they would",
"they'll": "they will",
"they're": "they are",
"they've": "they have",
"wasn't": "was not",
"we'd": "we would",
"we'll": "we will",
"we're": "we are",
"we've": "we have",
"weren't": "were not",
"what'll": "what will",
"what're": "what are",
"what's": "what is",
"what've": "what have",
"where'd": "where did",
"where's": "where is",
"who'll": "who will",
"who's": "who is",
"won't": "will not",
"wouldn't": "would not",
"you'd": "you would",
"you'll": "you will",
"you're": "you are"
}

# I added more stopwords based on a series of outputs. Got this code from our class notebook.
stop = STOPWORDS.union(set(['...', 'say', 'will', 'know', 'first', '\\/', '>>', '*.','|>',');','as', '10', 'be', ').', '--', 'edu', 'com','re', '."']))

# How many topics would you like
# Expirement with this number
num1 = 10

#Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings
def text_preprocessing(text):

  #makes all text lowercase
  text = text.lower()

  if True:
    text = text.split()
    new_text = []
    for word in text:
      if word not in stop:
        if word in contractions:
          new_text.append(contractions[word])
        else:
          new_text.append(word)
  text = " ".join(new_text)

  #formats words and removes unwanted characters
  text = re.sub(r'https?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE)
  text = re.sub(r'\'', ' ', text)
  text = re.sub(r'\'', ' ', text)

  #Tokenize each word
  text = nltk.WordPunctTokenizer().tokenize(text)
  text = [word for word in text if word not in stop] # Removing Stop words

  #Lemmatize each word
  text = [nltk.stem.WordNetLemmatizer().lemmatize(token, pos='v') for token in text if len(token)>1]

  return text

#convert list to string
def to_string(text):
  text = ' '.join(map(str, text))
  return text

docs = []
for article in articles:
  docs.append(text_preprocessing(article))

len(docs)

# pulled from our class notebook. gives each word its own unique integer ID.
dictionary = gensim.corpora.Dictionary(docs)


dictionary.filter_extremes(no_below=15, no_above=0.3, keep_n=100000)
vocab_length = len(dictionary)

bow_corpus = [dictionary.doc2bow(doc) for doc in docs]

gsdmm = MovieGroupProcess(K= num1, alpha=0.1, beta=0.9, n_iters=15)


y = gsdmm.fit(docs, vocab_length)

#let us print using the function above
doc_count = np.array(gsdmm.cluster_doc_count)
print('Number of documents per topic :', doc_count)
top_index = doc_count.argsort()[-15:][::-1]
print('Most important clusters (by number of docs inside):', top_index)
top_words(gsdmm.cluster_word_distribution, top_index, 20)

"""4. Create topic model using LDA"""

bow_corpus = [dictionary.doc2bow(doc) for doc in docs]

# Make an LDA model.
lda_model = models.LdaModel(corpus=bow_corpus, id2word=dictionary, num_topics=5, random_state=42, passes=10)

# Display topics
topics = lda_model.print_topics(num_words=10)
for topic in topics:
    print(topic)

"""5. Interpret the model.

Here, I have pulled 5 key topics from our articles.
- Topic 1, listed as 0, might be discussing a work environment, with repeated metions of "work," "mail," and "file," maybe emailing files or sharing between coworkers?
- Topic 2, listed as 1, appears to discuss the Christain religion. This can be inferred from the repeated mention of specific words such as "god," "jesus," "believe."
- Topic 3, listed as 2, appears to discuss a sports game of some sort, because or repeated metnions of "game," "play," and numbers which might represent score of the game.
- Topic 4, listed as 3, appears to discuss computers or their technologies, which can be inferred from repeated metions of "card," "drive," "pc," "mac," etc. Potentailly comparing the hardware of pc and macs?
- Topic 5, listed as 4, appears to disuss politics or laws/rights, based on the repeated mention of "gun," "state," and "government."

"""