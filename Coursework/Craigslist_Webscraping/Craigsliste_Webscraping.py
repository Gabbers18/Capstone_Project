# -*- coding: utf-8 -*-
"""Craigslist_Webscraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UDoftu3EK9G733_qByB1P-DI_p-o0mrz

**Week 5**

**WebScraping Craigslist**

Gabrielle Young
26 September 2024

**Part 1:**  Set up a pathway to access our Craigslist website for Garage and Moving Sales

1. Install Beautiful Soup Library and other necessary libraries for building CSV file later and adding sleep times.
"""

import requests
from bs4 import BeautifulSoup
import csv
import time

"""2. Create a pathway to the Craiglist Phoenix site looking at Garage & Moving Sales only.

3. Get data from the url using the "requests.get" function found in the beautiful soup documentation.
"""

cg_page = "https://phoenix.craigslist.org/search/sss?excats=5-2-13-22-2-24-1-23-1-1-1-1-1-1-3-6-10-1-1-1-2-2-8-1-1-1-1-1-4-1-3-1-3-1-1-1-1-7-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-2-1#search=1~gallery~0~0"
page = requests.get(cg_page)

"""4. Create Beautiful Soup opject that will use the specified Craigslist URL as an input."""

cg = BeautifulSoup(page.content, "html.parser")

"""5. Find anchors within this link."""

links = cg.find_all("a")

"""6. Filter to show only the "href", or the links to the individual listings for garage and moving sales."""

for link in links:
    href = link.get("href")
    if href:
        print(href)

"""**Part 2:** Search each individual link for our key words: "mattress," "cabinet," and "wrench."

1. Specifically identifity the 3 key words we will search each site for.

Initially, I put 'mattress OR cabinet OR wrench', but this was not working after mutliple tried. The code was pulling each letter out individually. This change was successful.
"""

keywords = ['mattress', 'cabinet', 'wrench']

"""2. Create an empty list to store results.

This will be used later when storing results. Chat GPT
"""

results = []

"""3. I wanted to use a function that would search each of these links for our set keywords.

Chat GPT helped me write this. I referred to the documentation, but I was having troubling figuring out how to search for specific keywords within multiple links. I spent a lot of time going through the documentation to find specific texts, but most of the time it was pulling from "title" or other areas of the websites I was not using, so I turned to Chat GPT. This code took like 15 minutes to run because there were around 300 links.
"""

# Chat GPT helped me display the process of searching within links and making
# sure the code was working for each link by printing out this explicit text.
# I was able to see errors for some links and if and where specific keywords
# were being pulled from
def search_hrefs_for_keywords(href, keywords):
    try:
        response = requests.get(href, timeout=10)
        response.raise_for_status()  # Raise an error for bad status codes

        page_content = response.text.lower()  # make lowercase

        # Chat GPT printed out this to see if code worked for each page
        print(f"Checking {href} for keywords...")

        # search keywords
        found_keywords = [keyword for keyword in keywords if keyword in page_content]

        # state and identify where keywords are found
        if found_keywords:
            results.append({"url": href, "keywords": ', '.join(found_keywords)})
            print(f"Found keywords: {found_keywords} in {href}")

    except requests.exceptions.RequestException as e:
        print(f"Error fetching {href}: {e}")

# function to search all links; here I reused the code from the previous section
# with modifications to search for keywords
for link in links:
    href = link.get("href")
    # Used ChatGPT to filter out links that did not work
    if href and 'cgi-bin' not in href:
        if href.startswith('/'):
            href = f"https://phoenix.craigslist.org{href}"

        search_hrefs_for_keywords(href, keywords)
        time.sleep(2)  # added sleep between each url for 2 seconds here

"""Write CSV file with keywords and links where they were each found. Chat GPT also helpped me with this. I tried to use the beautiful soup documentation in the write CSV file section, but it was not working well with the code above that I used. The "decompose" function was having issues."""

csv_file = "keywords_found.csv"
with open(csv_file, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.DictWriter(file, fieldnames=["url", "keywords"])
    writer.writeheader()
    writer.writerows(results)
